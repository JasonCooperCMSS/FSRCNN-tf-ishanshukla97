{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2723,
     "status": "ok",
     "timestamp": 1549733190072,
     "user": {
      "displayName": "Ishan Shukla",
      "photoUrl": "",
      "userId": "07111032317237931620"
     },
     "user_tz": -330
    },
    "id": "UDSgHscUupeE",
    "outputId": "47dde635-5a00-40ea-d66c-64c078424b0e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time\n",
    "from skimage import io\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(2)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import h5py\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2705,
     "status": "ok",
     "timestamp": 1549733190073,
     "user": {
      "displayName": "Ishan Shukla",
      "photoUrl": "",
      "userId": "07111032317237931620"
     },
     "user_tz": -330
    },
    "id": "aaFi8h4LupeJ",
    "outputId": "74c1dda9-a0b5-404a-8d74-b8e5792fa520"
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Args:\n",
    "    base: base folder name in which files are located\n",
    "    SRF: image_SRF folder to select\n",
    "    n_ids: number of ids to generated from 1...n_ids\n",
    "Returns:\n",
    "    A numpy character array of filenames\n",
    "\"\"\"\n",
    "def gen_filenames_labels(base, SRF, n_ids):\n",
    "    basepath = base + SRF\n",
    "    \n",
    "    dataset_arr = np.chararray(shape=(n_ids, 2), itemsize=128)\n",
    "    \n",
    "    for i in range(1, n_ids+1):\n",
    "        Y = basepath + 'img_{:03}_SRF_2_HR.png'.format(i)\n",
    "        X = basepath + 'img_{:03}_SRF_2_bicubic.png'.format(i)\n",
    "        X_dash = basepath + 'img_{:03}_SRF_2_LR.png'.format(i)\n",
    "        \n",
    "        dataset_arr[i-1, 0] = X_dash\n",
    "        dataset_arr[i-1, 1] = Y\n",
    "    \n",
    "    return dataset_arr\n",
    "\n",
    "dataset = gen_filenames_labels('train/', 'image_SRF_2/', 100)\n",
    "val_dataset = gen_filenames_labels('Set14/', 'image_SRF_2/', 5)\n",
    "\n",
    "X_train, X_test, y_train, y_test = dataset[:, 0], val_dataset[:, 0], dataset[:, 1], val_dataset[:, 1]\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BgC3gPnbupeN"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class to store parameters which would be required to build and train model\n",
    "\"\"\"\n",
    "\n",
    "class ModelParameters:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 256\n",
    "        self.input_height = 32\n",
    "        self.input_width = 32\n",
    "        self.label_height = 64\n",
    "        self.label_width = 64\n",
    "        self.n_channels = 3\n",
    "        self.n_epochs = 30\n",
    "        self.decay_rate = 0.9\n",
    "        self.decay_steps = 10000\n",
    "\n",
    "params = ModelParameters()\n",
    "\n",
    "'''\n",
    "def decode_ycbcr(image):\n",
    "    Y = image[0]\n",
    "    Cr = image[1]\n",
    "    Cb = image[2]\n",
    "    \n",
    "    delta = 0.5\n",
    "    \n",
    "    R = Y + 1.403 * (Cr - delta)\n",
    "    G = Y - 0.714 * (Cr - delta) - 0.344 * (Cb - delta)\n",
    "    B = Y + 1.779 * (Cb - delta)\n",
    "    \n",
    "    ycrcb = np.array([R,G,B])\n",
    "    \n",
    "    rgb[0], rgb[1], rgb[2] = R, G, B\n",
    "    return image\n",
    "'''\n",
    "\n",
    "def encode_ycbcr(image):\n",
    "    R = image[:, :, 0]\n",
    "    G = image[:, :, 1]\n",
    "    B = image[:, :, 2]\n",
    "    \n",
    "    delta = 0.5\n",
    "    \n",
    "    Y = tf.add(tf.add(tf.multiply(0.299, R), tf.multiply(0.587, G)), tf.multiply(0.114, B))\n",
    "    Cr = tf.add(tf.multiply(tf.subtract(R, Y),0.713), delta)\n",
    "    Cb = tf.add(tf.multiply(tf.subtract(B, Y), 0.564), delta)\n",
    "    \n",
    "    img = tf.stack([Y, Cr, Cb], axis=2)\n",
    "    return img\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Builds a tensorflow graph to be used for loading files into memory. ETL process\n",
    "\n",
    "Args:\n",
    "    filename: filename\n",
    "    ksizes: A list of ints that has length >= 4. \n",
    "            The size of the sliding window for each dimension of images\n",
    "    strides: A list of ints that has length >= 4. \n",
    "            1-D of length 4. How far the centers of two consecutive patches are in the images. \n",
    "            Must be: [1, stride_rows, stride_cols, 1].\n",
    "    rates: A list of ints that has length >= 4. 1-D of length 4. \n",
    "            Must be: [1, rate_rows, rate_cols, 1]. \n",
    "            This is the input stride, specifying how far two consecutive patch samples are in the input. \n",
    "            Equivalent to extracting patches with patch_sizes_eff = patch_sizes + (patch_sizes - 1) * (rates - 1), \n",
    "            followed by subsampling them spatially by a factor of rates. \n",
    "            This is equivalent to rate in dilated (a.k.a. Atrous) convolutions.\n",
    "            \n",
    "Returns:\n",
    "    Batch of patches extracted from the image.\n",
    "\"\"\"\n",
    "def get_img_from_file(filename, ksizes, kstrides, rates, height, width, channels, color_space='YCbCr'):\n",
    "    file = tf.read_file(filename)\n",
    "    img = tf.image.decode_png(file, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    \n",
    "    if color_space is 'YCbCr':\n",
    "        img = encode_ycbcr(img)\n",
    "    \n",
    "    patches = tf.image.extract_image_patches(tf.expand_dims(img, 0), \n",
    "                                             ksizes, \n",
    "                                             kstrides, \n",
    "                                             rates, \n",
    "                                             padding='VALID')\n",
    "\n",
    "    input_img_batch = tf.squeeze(patches)\n",
    "    shape = tf.shape(input_img_batch)\n",
    "    input_img_batch = tf.reshape(input_img_batch, [shape[0]*shape[1], \n",
    "                                                   height, \n",
    "                                                   width, \n",
    "                                                   channels])\n",
    "    return input_img_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5sXDJ3HZupeQ"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A class for building graph that loads and initializes iterators for loading data into graph\n",
    "\"\"\"\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, params):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.params = params\n",
    "        self.in_ksizes = [1, params.input_height, params.input_width, 1]\n",
    "        self.out_ksizes = [1, params.label_height, params.label_width, 1]\n",
    "        self.in_kstrides = [1, 10, 10, 1]\n",
    "        self.out_kstrides = [1, 20, 20, 1]\n",
    "        self.rates = [1, 1, 1, 1]\n",
    "        \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        num_threads: number of parallel calls to be made while parsing\n",
    "        num_prefetch: number of batches to be pre loaded before training\n",
    "        \n",
    "    Returns:\n",
    "        Next batch to be served to model\n",
    "    \n",
    "    \"\"\"\n",
    "    def build_iterators(self, num_threads=8, num_prefetch=8):\n",
    "        def parse_fn(filename, label):\n",
    "            input_img_batch = get_img_from_file(filename, \n",
    "                                                self.in_ksizes, \n",
    "                                                self.in_kstrides, \n",
    "                                                self.rates, \n",
    "                                                self.params.input_height, \n",
    "                                                self.params.input_width, \n",
    "                                                self.params.n_channels)\n",
    "\n",
    "            ground_img_batch = get_img_from_file(label, \n",
    "                                                 self.out_ksizes, \n",
    "                                                 self.out_kstrides, \n",
    "                                                 self.rates, \n",
    "                                                 self.params.label_height, \n",
    "                                                 self.params.label_width, \n",
    "                                                 self.params.n_channels)\n",
    "\n",
    "            return [input_img_batch, ground_img_batch]\n",
    "        \n",
    " \n",
    "        #train dataset graph\n",
    "        filenames = self.X_train\n",
    "        labels = self.y_train\n",
    "        \n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "        train_dataset = train_dataset.shuffle(len(filenames))\n",
    "        train_dataset = train_dataset.map(parse_fn, num_parallel_calls=num_threads)\n",
    "        train_dataset = train_dataset.apply(tf.contrib.data.unbatch())\n",
    "\n",
    "        train_dataset = train_dataset.batch(self.params.batch_size)\n",
    "        self.train_dataset = train_dataset.prefetch(num_prefetch)\n",
    "        \n",
    "        #val dataset graph\n",
    "        filenames = self.X_test\n",
    "        labels = self.y_test\n",
    "        \n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "        val_dataset = val_dataset.shuffle(len(filenames))\n",
    "        val_dataset = val_dataset.map(parse_fn, num_parallel_calls=num_threads)\n",
    "        val_dataset = val_dataset.apply(tf.contrib.data.unbatch())\n",
    "        val_dataset = val_dataset.batch(self.params.batch_size)\n",
    "        self.val_dataset = val_dataset.prefetch(num_prefetch)\n",
    "\n",
    "        #Make iterator\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types, \n",
    "                                                   train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "        \n",
    "        self.train_init_op = iterator.make_initializer(self.train_dataset)\n",
    "        self.val_init_op = iterator.make_initializer(self.val_dataset)\n",
    "        \n",
    "        return next_element\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8UVo8hv9upeS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.image import extract_patches_2d\n",
    "from tqdm import tqdm\n",
    "from model import FSRCNN\n",
    "\"\"\"\n",
    "Model class for training and inference\n",
    "Contains:\n",
    "    build(): Builds the graphs\n",
    "    fit(): trains the network for params.n_epochs\n",
    "    score(): For scoring PSNR and loss on validation dataset\n",
    "    predict(): Takes the filename as arg and returns prediction.\n",
    "\"\"\"\n",
    "class Model:\n",
    "    def __init__(self, params, data_loader):\n",
    "        self.params = params\n",
    "        self.data_loader = data_loader\n",
    "        self.d = 32\n",
    "        self.s = 8\n",
    "        self.m = 1\n",
    "        \n",
    "        #Set mode to 'train' for training and 'infer' for prediction on your own images.\n",
    "        #Set this parameter before running the build function.\n",
    "        self.mode = 'train'\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    Builds the training and inference graphs\n",
    "        \n",
    "    \"\"\"\n",
    "    def build(self):\n",
    "        decay_rate = self.params.decay_rate\n",
    "        decay_steps = self.params.decay_steps\n",
    "        \n",
    "        next_element = self.data_loader.build_iterators()\n",
    "        self.y = next_element[1]\n",
    "        \n",
    "        gt_image_summ = tf.summary.image('gt_image', self.y)\n",
    "        \n",
    "        with tf.name_scope('inference'):\n",
    "            self.input_img_placeholder = tf.placeholder(tf.float32, shape=(None, self.params.input_height, \n",
    "                                                                           self.params.input_width, \n",
    "                                                                           self.params.n_channels))\n",
    "\n",
    "        \n",
    "        with tf.name_scope('convolutional'):\n",
    "            if self.mode == 'train':\n",
    "                x = next_element[0]\n",
    "                input_image_summ = tf.summary.image('input_image', x)\n",
    "                \n",
    "            elif self.mode == 'infer':\n",
    "                x = self.input_img_placeholder\n",
    "                \n",
    "            x, self.model_summ = FSRCNN(x)\n",
    "            \n",
    "            self.output = x\n",
    "            \n",
    "            frac = self.output.get_shape().as_list()[1]/self.y.get_shape().as_list()[1]\n",
    "            print(frac, self.output.get_shape().as_list(), self.y.get_shape().as_list()[1])\n",
    "            self.y = tf.image.central_crop(self.y, central_fraction=frac)\n",
    "            pred_image_summ = tf.summary.image('pred_image', x)\n",
    "                        \n",
    "        with tf.name_scope('loss'):\n",
    "            \n",
    "            self.loss = tf.losses.mean_squared_error(self.y, self.output)\n",
    "            \n",
    "        with tf.name_scope('metrics'):\n",
    "            self.psnr = tf.reduce_mean(tf.image.psnr(self.output, self.y, max_val=1))\n",
    "            self.ssim = tf.reduce_mean(tf.image.ssim(self.output, self.y, max_val=1))\n",
    "        \n",
    "        with tf.name_scope('train'):\n",
    "            self.g_step_tensor = tf.Variable(0, trainable=False)\n",
    "            self.learning_rate = tf.placeholder(tf.float32, shape=None, name='learning_rate')\n",
    "            init_lr = self.learning_rate\n",
    "            decayed_lr = tf.train.exponential_decay(init_lr, self.g_step_tensor,\n",
    "                                                    decay_steps, decay_rate, staircase=True)\n",
    "            lr_summ = tf.summary.scalar('lr', decayed_lr)\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer(decayed_lr)\n",
    "            self.grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
    "            self.train_op = self.optimizer.apply_gradients(self.grads_and_vars, global_step=self.g_step_tensor)\n",
    "             \n",
    "        with tf.name_scope('performance'):\n",
    "            \n",
    "            train_loss_summ = tf.summary.scalar('train_loss', self.loss)\n",
    "            train_psnr_summ = tf.summary.scalar('train_psnr', self.psnr)\n",
    "            train_ssim_summ = tf.summary.scalar('train_ssim', self.ssim)\n",
    "            self.train_stats = tf.summary.merge([train_loss_summ, \n",
    "                                            train_psnr_summ, \n",
    "                                            train_ssim_summ])\n",
    "            \n",
    "            valid_loss_summ = tf.summary.scalar('valid_loss', self.loss)\n",
    "            valid_psnr_summ = tf.summary.scalar('valid_psnr', self.psnr)\n",
    "            valid_ssim_summ = tf.summary.scalar('valid_ssim', self.ssim)\n",
    "            self.valid_stats = tf.summary.merge([valid_loss_summ, \n",
    "                                            valid_psnr_summ, \n",
    "                                            valid_ssim_summ])\n",
    "            grads_summ = []\n",
    "            l2_norm = lambda t: tf.sqrt(tf.reduce_sum(tf.pow(t, 2)))\n",
    "            \n",
    "            for gv in self.grads_and_vars:\n",
    "                if 'conv2d' in gv[1].name:\n",
    "                    name = gv[1].name.replace(':', '_')\n",
    "                    grads_summ.append(tf.summary.scalar(name, l2_norm(gv[1])))\n",
    "                    \n",
    "            self.performance_summ = tf.summary.merge([grads_summ, lr_summ])\n",
    "            self.image_stats = tf.summary.merge([gt_image_summ, \n",
    "                                                 input_image_summ, \n",
    "                                                 pred_image_summ])\n",
    "\n",
    "    def fit(self, lr, summ_writer):\n",
    "        epochs = self.params.n_epochs\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\n",
    "            #saver.restore(sess, 'dense_models/srcnn_0.0007.ckpt')\n",
    "            summ_writer.add_graph(sess.graph)\n",
    "            \n",
    "            val_losses = [1000.0]\n",
    "            for epoch in range(epochs):\n",
    "                train_len = 102\n",
    "                loss_all = []\n",
    "                psnr_all = []\n",
    "                \n",
    "                \n",
    "                print('\\n**************************')\n",
    "                print('Epoch: ' + str(epoch))\n",
    "                with tqdm(total=train_len) as pbar:\n",
    "                    sess.run(self.data_loader.train_init_op)\n",
    "                    \n",
    "                    for steps in range(train_len):\n",
    "                        train, loss, psnr, mod_summ, train_summ, perf_summ, image_summ = sess.run([self.train_op, \n",
    "                                                                                         self.loss, \n",
    "                                                                                         self.psnr, \n",
    "                                                                                         self.model_summ,\n",
    "                                                                                         self.train_stats, \n",
    "                                                                                         self.performance_summ,\n",
    "                                                                                         self.image_stats], \n",
    "                                                                                         feed_dict={self.learning_rate: lr})\n",
    "                        \n",
    "                        \n",
    "                        summ_writer.add_summary(train_summ, global_step=tf.train.global_step(sess, self.g_step_tensor))\n",
    "                        summ_writer.add_summary(perf_summ, global_step=tf.train.global_step(sess, self.g_step_tensor))\n",
    "                        summ_writer.add_summary(image_summ, global_step=tf.train.global_step(sess, self.g_step_tensor))\n",
    "                        summ_writer.add_summary(mod_summ, global_step=tf.train.global_step(sess, self.g_step_tensor))\n",
    "                        \n",
    "                        loss_all.append(loss)\n",
    "                        psnr_all.append(psnr)\n",
    "                        pbar.set_description('loss: {:.4f} -- psnr: {:.4f}'.format(float(loss), psnr))\n",
    "                        pbar.update(1)\n",
    "                    \n",
    "                print('train_loss: {:.4f} -- train_psnr: {:.4f}'.format(np.mean(loss_all), \n",
    "                                                                        np.mean(psnr_all)))\n",
    "                \n",
    "\n",
    "                print('lr:{}'.format(lr))\n",
    "                \n",
    "                \n",
    "                val_loss = self.score(sess)\n",
    "                valid_summ = sess.run(self.valid_stats)\n",
    "                summ_writer.add_summary(valid_summ, global_step=tf.train.global_step(sess, self.g_step_tensor))\n",
    "                \n",
    "                if val_loss < min(val_losses):\n",
    "                    print('Saving model with val_loss: {:.4f} to file: unnamed_cnn_{:.4f}.ckpt'.format(val_loss,\n",
    "                                                                                                 val_loss))\n",
    "                    saver.save(sess, 'dense_models/unnamed_cnn_{:.4f}.ckpt'.format(val_loss))\n",
    "                else:\n",
    "                    print('val_loss did not improve from last best: {:.4}'.format(min(val_losses)))\n",
    "                print('**************************\\n')\n",
    "                val_losses.append(val_loss)\n",
    "\n",
    "    def score(self, session):\n",
    "\n",
    "        with session.as_default():\n",
    "\n",
    "            test_len = 3\n",
    "\n",
    "            loss_all = []\n",
    "            psnr_all = []\n",
    "\n",
    "            with tqdm(total=test_len) as pbar:\n",
    "                session.run(self.data_loader.val_init_op)\n",
    "\n",
    "                for steps in range(test_len):\n",
    "                    loss, psnr = session.run([self.loss, self.psnr])\n",
    "\n",
    "                    loss_all.append(loss)\n",
    "                    psnr_all.append(psnr)\n",
    "\n",
    "                    pbar.set_description('val_loss: {:.4f} -- val_psnr: {:.4f}'.format(loss, psnr))\n",
    "                    pbar.update(1)\n",
    "\n",
    "            print('val_loss: ' + str(np.mean(loss_all)), ' -- val_psnr: ' + str(np.mean(psnr_all)))\n",
    "            return np.mean(loss_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NYktgrDfupeU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_loader = DataLoader(X_train, y_train, X_test, y_test, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4775,
     "status": "ok",
     "timestamp": 1549733192220,
     "user": {
      "displayName": "Ishan Shukla",
      "photoUrl": "",
      "userId": "07111032317237931620"
     },
     "user_tz": -330
    },
    "id": "bwZgsUoUupeX",
    "outputId": "99c373fe-cbca-4e35-b901-5d20ae7fa1eb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model(params, data_loader)\n",
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nkismnMEupea",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Lr search\n",
    "for lr in [0.001, 0.002, 0.003, 0.004, 0.005]:\n",
    "    hparam_str = 'lr_{:}'.format(lr)\n",
    "    writer = tf.summary.FileWriter('summaries/prelu/' + hparam_str)\n",
    "    model.fit(lr, writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XAYwXm_tupef"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5yv50USUupei",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oiFDc85Zupel",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FCYDt7s0upem"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fEMkSh2Zuper"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ju61otC-upeu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PxoA7VlEupew"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WeuNfZw9upez"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AlMFdcM5vRDx",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#I run these cells to train the network on Colab\n",
    "#Downloads train and test sets and sets up tensorboard. Run cells in order\n",
    "\n",
    "#Step 1 - download \n",
    "\n",
    "#!wget \"https://uofi.box.com/shared/static/kfahv87nfe8ax910l85dksyl2q212voc.zip\" Set5\n",
    "!wget \"https://uofi.box.com/shared/static/65upg43jjd0a4cwsiqgl6o6ixube6klm.zip\"\n",
    "!wget \"https://uofi.box.com/shared/static/igsnfieh4lz68l926l8xbklwsnnk8we9.zip\"\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "for file in os.listdir():\n",
    "  print(file)\n",
    "  if file == 'igsnfieh4lz68l926l8xbklwsnnk8we9.zip' or file == '65upg43jjd0a4cwsiqgl6o6ixube6klm.zip':\n",
    "    zip_ref = zipfile.ZipFile(file, 'r')\n",
    "    zip_ref.extractall()\n",
    "    zip_ref.close()\n",
    "    \n",
    "\n",
    "!mkdir train\n",
    "!mv image_SRF_2 train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2\n",
    "!mkdir 'summaries'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4yRupsHuwAhB"
   },
   "outputs": [],
   "source": [
    "#Step 3\n",
    "#Tensorboard\n",
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "!unzip ngrok-stable-linux-amd64.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OEV8VDbUwxv3"
   },
   "outputs": [],
   "source": [
    "#Step 4\n",
    "LOG_DIR = './summaries/prelu'\n",
    "get_ipython().system_raw(\n",
    "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
    "    .format(LOG_DIR)\n",
    ")\n",
    "get_ipython().system_raw('./ngrok http 6006 &')\n",
    "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pa4bmHQLxgCS"
   },
   "outputs": [],
   "source": [
    "!rm -r summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O80-CC4IhuKc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YkD8zZUhQIJ3",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yH_HwXu10DmF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "super_res_1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
